{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import r2_score\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the path to the folder \n",
    "dir=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths to data \n",
    "albedo=dir+\"/DATA/Data_Albedo/Albedo_Map.csv\"\n",
    "LPFe_Map=dir+\"/DATA/Data_Albedo/LPFe_Map.csv\"\n",
    "LPK_Map=dir+\"/DATA/Data_Albedo/LPK_Map.csv\"\n",
    "LPTh_Map=dir+\"/DATA/Data_Albedo/LPTh_Map.csv\"\n",
    "LPTi_Map=dir+\"/DATA/Data_Albedo/LPTi_Map.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom dataset for nearby pixels\n",
    "class dataset_nearby_pixel(Dataset):\n",
    "    def __init__(self,path_1,path_2,path_3,path_4,path_5,mode):\n",
    "        self.path_1=path_1\n",
    "        self.path_2=path_2\n",
    "        self.path_3=path_3\n",
    "        self.path_4=path_4\n",
    "        self.path_5=path_5\n",
    "        self.mode=mode\n",
    "        #converting the data to ndarray\n",
    "        self.X_1=np.array(pd.read_csv(path_1))\n",
    "        self.X_2=np.array(pd.read_csv(path_2))\n",
    "        self.X_3=np.array(pd.read_csv(path_3))\n",
    "        self.X_4=np.array(pd.read_csv(path_4))\n",
    "        self.X_5=cv2.GaussianBlur(np.array(pd.read_csv(path_5)),ksize=(0,0),sigmaX=9)\n",
    "        self.X=[]\n",
    "        self.Y=[]\n",
    "        n,m=np.shape(self.X_5)[0],np.shape(self.X_5)[1]\n",
    "        for i in range(1,n-1):\n",
    "            for j in range(1,m-1):\n",
    "                nearby=[]\n",
    "                for chem in [self.X_1,self.X_2,self.X_3,self.X_4]:\n",
    "                    for a in [-1,0,1]:\n",
    "                        for b in [-1,0,1]:\n",
    "                            nearby.append(chem[i+a][j+b])\n",
    "                self.X.append(nearby)\n",
    "                self.Y.append([self.X_5[i][j]])\n",
    "        l=len(self.Y)//2\n",
    "        if mode == \"Train\" or \"train\":\n",
    "            self.X=np.array(self.X)[:l]\n",
    "            self.Y=np.array(self.Y)[:l]\n",
    "        elif mode== \"Test\" or \"test\":\n",
    "            self.X=np.array(self.X)[l:]\n",
    "            self.Y=np.array(self.Y)[l:]\n",
    "    def __len__(self):\n",
    "        self.filelength=np.shape(self.Y)[0]\n",
    "        return self.filelength\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return torch.from_numpy(self.X[idx]),torch.from_numpy(self.Y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        # Encoder Network\n",
    "        self.encoder = torch.nn.Sequential(torch.nn.Linear(36,72),\n",
    "                                     torch.nn.ReLU(True),\n",
    "                                     torch.nn.Linear(72,148))\n",
    "        # Decoder Network\n",
    "        self.decoder = torch.nn.Sequential(torch.nn.Linear(148,72),\n",
    "                                     torch.nn.ReLU(True),\n",
    "                                     torch.nn.Linear(72, 36),\n",
    "                                     torch.nn.ReLU(True),\n",
    "                                     torch.nn.Linear(36,1),\n",
    "                                     torch.nn.ReLU(True))\n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train():\n",
    "    def __init__(self,batch_size,epochs,lr,train_val_split,scheduler,near):\n",
    "        self.batch_size=batch_size\n",
    "        self.scheduler=scheduler\n",
    "        self.epochs=epochs\n",
    "        self.lr=lr\n",
    "        self.train_val_split=train_val_split\n",
    "        self.near=near\n",
    "        if self.near== True:\n",
    "            self.data=dataset_nearby_pixel(LPFe_Map,LPK_Map,LPTh_Map,LPTi_Map,albedo,mode=\"train\")\n",
    "            self.train_data,self.val_data=random_split(self.data,[len(self.data)-int(self.train_val_split*len(self.data)),int(self.train_val_split*len(self.data))],generator=torch.Generator().manual_seed(42))\n",
    "            self.train_loader=DataLoader(self.train_data,batch_size=self.batch_size,shuffle=True)\n",
    "            self.val_loader=DataLoader(self.val_data,batch_size=self.batch_size,shuffle=True)\n",
    "            self.net=autoencoder()\n",
    "        # else:\n",
    "        #     self.data=dataset(LPFe_Map,LPK_Map,LPTh_Map,LPTi_Map,albedo,0,360)\n",
    "        #     self.train_data,self.val_data=random_split(self.data,[len(self.data)-int(self.train_val_split*len(self.data)),int(self.train_val_split*len(self.data))],generator=torch.Generator().manual_seed(42))\n",
    "        #     self.train_loader=DataLoader(self.train_data,batch_size=self.batch_size,shuffle=True)\n",
    "        #     self.val_loader=DataLoader(self.val_data,batch_size=self.batch_size,shuffle=True)\n",
    "        #     self.net=model(n_feature=4, n_hidden=4, n_output=1)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        if self.scheduler==True:\n",
    "            self.sched=torch.optim.lr_scheduler.ExponentialLR(self.optimizer,gamma=0.7)\n",
    "        self.loss_func = torch.nn.MSELoss() \n",
    "        self.writer = SummaryWriter()\n",
    "        \n",
    "    def trainer(self):\n",
    "        self.net=self.net.train()\n",
    "        self.net=self.net.cuda()\n",
    "        for epoch in range(self.epochs):\n",
    "            for input,gt in self.train_loader:\n",
    "                input = input.cuda()\n",
    "                gt = gt.cuda()\n",
    "                gt=torch.reshape(gt,(len(gt),1))\n",
    "                output = self.net(input.float())\n",
    "                loss = self.loss_func(output, gt.float())\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            if self.scheduler == True:\n",
    "                self.sched.step()\n",
    "            print('Epoch : {},  train loss : {}'.format(epoch+1,loss.item()))\n",
    "            with torch.no_grad():\n",
    "                for input,gt in self.val_loader:\n",
    "                    input=input.cuda()\n",
    "                    gt= gt.cuda()\n",
    "                    gt=torch.reshape(gt,(len(gt),1))\n",
    "                    val_output = self.net(input.float())\n",
    "                    val_loss = self.loss_func(val_output,gt.float())\n",
    "            print('Epoch : {},  val_loss : {}'.format(epoch+1,val_loss.item()))\n",
    "            self.writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "            self.writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "            if self.scheduler == True:\n",
    "                self.writer.add_scalar(\"lr/epoch\",self.lr,epoch)\n",
    "        torch.save(self.net.state_dict(),f\"albedo_autoencoder_{self.epochs}_{self.lr}_{self.batch_size}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 16:56:31.090501: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/g0kul6/miniconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-10 16:56:31.090532: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1,  train loss : 0.0878107100725174\n",
      "Epoch : 1,  val_loss : 0.09150587022304535\n",
      "Epoch : 2,  train loss : 0.08635705709457397\n",
      "Epoch : 2,  val_loss : 0.08455988019704819\n",
      "Epoch : 3,  train loss : 0.08399862796068192\n",
      "Epoch : 3,  val_loss : 0.09489002823829651\n",
      "Epoch : 4,  train loss : 0.08470350503921509\n",
      "Epoch : 4,  val_loss : 0.0816386342048645\n",
      "Epoch : 5,  train loss : 0.07871576398611069\n",
      "Epoch : 5,  val_loss : 0.07830406725406647\n",
      "Epoch : 6,  train loss : 0.08545029908418655\n",
      "Epoch : 6,  val_loss : 0.08926884829998016\n",
      "Epoch : 7,  train loss : 0.07614102959632874\n",
      "Epoch : 7,  val_loss : 0.08242516964673996\n",
      "Epoch : 8,  train loss : 0.001455507823266089\n",
      "Epoch : 8,  val_loss : 0.0037453914992511272\n",
      "Epoch : 9,  train loss : 0.002118888543918729\n",
      "Epoch : 9,  val_loss : 0.0018569575622677803\n",
      "Epoch : 10,  train loss : 0.000737999624107033\n",
      "Epoch : 10,  val_loss : 0.0023089596070349216\n",
      "Epoch : 11,  train loss : 0.0009930769447237253\n",
      "Epoch : 11,  val_loss : 0.0012720798840746284\n",
      "Epoch : 12,  train loss : 0.00169272068887949\n",
      "Epoch : 12,  val_loss : 0.003894631750881672\n",
      "Epoch : 13,  train loss : 0.0007690248312428594\n",
      "Epoch : 13,  val_loss : 0.0005912671331316233\n",
      "Epoch : 14,  train loss : 0.0006027873605489731\n",
      "Epoch : 14,  val_loss : 0.0033615445718169212\n",
      "Epoch : 15,  train loss : 0.00039357933565042913\n",
      "Epoch : 15,  val_loss : 0.001383837079629302\n",
      "Epoch : 16,  train loss : 0.00035889967693947256\n",
      "Epoch : 16,  val_loss : 0.0006209786515682936\n",
      "Epoch : 17,  train loss : 0.0010290767531841993\n",
      "Epoch : 17,  val_loss : 0.00023587308533024043\n",
      "Epoch : 18,  train loss : 0.00041479113860987127\n",
      "Epoch : 18,  val_loss : 0.0005015709903091192\n",
      "Epoch : 19,  train loss : 0.0005539472913369536\n",
      "Epoch : 19,  val_loss : 0.0004221520503051579\n",
      "Epoch : 20,  train loss : 0.0004449983825907111\n",
      "Epoch : 20,  val_loss : 0.0005475503276102245\n",
      "Epoch : 21,  train loss : 0.0003932579420506954\n",
      "Epoch : 21,  val_loss : 0.000538503285497427\n",
      "Epoch : 22,  train loss : 0.0005059874383732677\n",
      "Epoch : 22,  val_loss : 0.000414421025197953\n",
      "Epoch : 23,  train loss : 0.00033786913263611495\n",
      "Epoch : 23,  val_loss : 0.00037795904790982604\n",
      "Epoch : 24,  train loss : 0.0005521064158529043\n",
      "Epoch : 24,  val_loss : 0.0005237184232100844\n",
      "Epoch : 25,  train loss : 0.00031337534892372787\n",
      "Epoch : 25,  val_loss : 0.0003929006343241781\n",
      "Epoch : 26,  train loss : 0.000631699978839606\n",
      "Epoch : 26,  val_loss : 0.0003714418853633106\n",
      "Epoch : 27,  train loss : 0.0004454639565665275\n",
      "Epoch : 27,  val_loss : 0.0003284956910647452\n",
      "Epoch : 28,  train loss : 0.0003302760305814445\n",
      "Epoch : 28,  val_loss : 0.00032676703995093703\n",
      "Epoch : 29,  train loss : 0.00031398964347317815\n",
      "Epoch : 29,  val_loss : 0.0002015146892517805\n",
      "Epoch : 30,  train loss : 0.0003893116081599146\n",
      "Epoch : 30,  val_loss : 0.00039379074587486684\n",
      "Epoch : 31,  train loss : 0.0002021557738771662\n",
      "Epoch : 31,  val_loss : 0.0003710713644977659\n",
      "Epoch : 32,  train loss : 0.00042482971912249923\n",
      "Epoch : 32,  val_loss : 0.00040611831354908645\n",
      "Epoch : 33,  train loss : 0.00035993478377349675\n",
      "Epoch : 33,  val_loss : 0.00027809111634269357\n",
      "Epoch : 34,  train loss : 0.0003080436435993761\n",
      "Epoch : 34,  val_loss : 0.0004221129638608545\n",
      "Epoch : 35,  train loss : 0.00034128897823393345\n",
      "Epoch : 35,  val_loss : 0.00028266041772440076\n",
      "Epoch : 36,  train loss : 0.0003469839575700462\n",
      "Epoch : 36,  val_loss : 0.0002876578364521265\n",
      "Epoch : 37,  train loss : 0.00029542899574153125\n",
      "Epoch : 37,  val_loss : 0.00028885542997159064\n",
      "Epoch : 38,  train loss : 0.00021320061932783574\n",
      "Epoch : 38,  val_loss : 0.00031388140632770956\n",
      "Epoch : 39,  train loss : 0.0002388346183579415\n",
      "Epoch : 39,  val_loss : 0.0002876596408896148\n",
      "Epoch : 40,  train loss : 0.00031169658177532256\n",
      "Epoch : 40,  val_loss : 0.00019342736050020903\n",
      "Epoch : 41,  train loss : 0.00036268055555410683\n",
      "Epoch : 41,  val_loss : 0.0002429357700748369\n",
      "Epoch : 42,  train loss : 0.0003934462438337505\n",
      "Epoch : 42,  val_loss : 0.00018068814824800938\n",
      "Epoch : 43,  train loss : 0.00029094802448526025\n",
      "Epoch : 43,  val_loss : 0.00024218449834734201\n",
      "Epoch : 44,  train loss : 0.00019266123126726598\n",
      "Epoch : 44,  val_loss : 0.00022934206936042756\n",
      "Epoch : 45,  train loss : 0.0002391851885477081\n",
      "Epoch : 45,  val_loss : 0.0003555946168489754\n",
      "Epoch : 46,  train loss : 0.0003869547217618674\n",
      "Epoch : 46,  val_loss : 0.00028652395121753216\n",
      "Epoch : 47,  train loss : 0.00025124201783910394\n",
      "Epoch : 47,  val_loss : 0.000255148479482159\n",
      "Epoch : 48,  train loss : 0.00028794832178391516\n",
      "Epoch : 48,  val_loss : 0.00014857907081022859\n",
      "Epoch : 49,  train loss : 0.00014865322737023234\n",
      "Epoch : 49,  val_loss : 0.00018187488603871316\n",
      "Epoch : 50,  train loss : 0.00023415272880811244\n",
      "Epoch : 50,  val_loss : 0.0003235582262277603\n",
      "Epoch : 51,  train loss : 0.0002520301495678723\n",
      "Epoch : 51,  val_loss : 0.00019141320080962032\n",
      "Epoch : 52,  train loss : 0.0002367224806221202\n",
      "Epoch : 52,  val_loss : 0.00024819208192639053\n",
      "Epoch : 53,  train loss : 0.00017335719894617796\n",
      "Epoch : 53,  val_loss : 0.00022475211881101131\n",
      "Epoch : 54,  train loss : 0.00018994287529494613\n",
      "Epoch : 54,  val_loss : 0.00023951800540089607\n",
      "Epoch : 55,  train loss : 0.0003508939116727561\n",
      "Epoch : 55,  val_loss : 0.00020230821974109858\n",
      "Epoch : 56,  train loss : 0.0003284876001998782\n",
      "Epoch : 56,  val_loss : 0.000191724015166983\n",
      "Epoch : 57,  train loss : 0.00027401631814427674\n",
      "Epoch : 57,  val_loss : 0.00017758674221113324\n",
      "Epoch : 58,  train loss : 0.00018351306789554656\n",
      "Epoch : 58,  val_loss : 0.00027551455423235893\n",
      "Epoch : 59,  train loss : 0.00029632699443027377\n",
      "Epoch : 59,  val_loss : 0.00026373896980658174\n",
      "Epoch : 60,  train loss : 0.00028729852056130767\n",
      "Epoch : 60,  val_loss : 0.00023372776922769845\n",
      "Epoch : 61,  train loss : 0.00016634882194921374\n",
      "Epoch : 61,  val_loss : 0.00023168826010078192\n",
      "Epoch : 62,  train loss : 0.00020327151287347078\n",
      "Epoch : 62,  val_loss : 0.00021067260240670294\n",
      "Epoch : 63,  train loss : 0.00019967790285591036\n",
      "Epoch : 63,  val_loss : 0.00013012014096602798\n",
      "Epoch : 64,  train loss : 0.0001605566794751212\n",
      "Epoch : 64,  val_loss : 0.00034965912345796824\n",
      "Epoch : 65,  train loss : 0.00018572727276477963\n",
      "Epoch : 65,  val_loss : 0.00019503121438901871\n",
      "Epoch : 66,  train loss : 0.0001818834716686979\n",
      "Epoch : 66,  val_loss : 0.00017112800560425967\n",
      "Epoch : 67,  train loss : 0.0003523861523717642\n",
      "Epoch : 67,  val_loss : 0.0002301791391801089\n",
      "Epoch : 68,  train loss : 0.0001988007134059444\n",
      "Epoch : 68,  val_loss : 0.00015962244651746005\n",
      "Epoch : 69,  train loss : 0.0002651807735674083\n",
      "Epoch : 69,  val_loss : 0.00022040950716473162\n",
      "Epoch : 70,  train loss : 0.0001922467927215621\n",
      "Epoch : 70,  val_loss : 0.00017349841073155403\n",
      "Epoch : 71,  train loss : 0.0001790209935279563\n",
      "Epoch : 71,  val_loss : 0.00025164944236166775\n",
      "Epoch : 72,  train loss : 0.0003164437075611204\n",
      "Epoch : 72,  val_loss : 0.00028819861472584307\n",
      "Epoch : 73,  train loss : 0.00026409985730424523\n",
      "Epoch : 73,  val_loss : 0.0001858785399235785\n",
      "Epoch : 74,  train loss : 0.0001594211789779365\n",
      "Epoch : 74,  val_loss : 0.0002197386638727039\n",
      "Epoch : 75,  train loss : 0.00031874972046352923\n",
      "Epoch : 75,  val_loss : 0.00021966306667309254\n",
      "Epoch : 76,  train loss : 0.00023409670393448323\n",
      "Epoch : 76,  val_loss : 0.00023453112225979567\n",
      "Epoch : 77,  train loss : 0.00021693574672099203\n",
      "Epoch : 77,  val_loss : 0.0001339162845397368\n",
      "Epoch : 78,  train loss : 0.00028131663566455245\n",
      "Epoch : 78,  val_loss : 0.00014900673704687506\n",
      "Epoch : 79,  train loss : 0.00013375085836742073\n",
      "Epoch : 79,  val_loss : 0.00017710859538055956\n",
      "Epoch : 80,  train loss : 0.0002089966437779367\n",
      "Epoch : 80,  val_loss : 0.00027121807215735316\n",
      "Epoch : 81,  train loss : 0.00011807464034063742\n",
      "Epoch : 81,  val_loss : 0.0001936611661221832\n",
      "Epoch : 82,  train loss : 0.00026545108994469047\n",
      "Epoch : 82,  val_loss : 0.00018120225286111236\n",
      "Epoch : 83,  train loss : 0.00030542834429070354\n",
      "Epoch : 83,  val_loss : 0.00016551249427720904\n",
      "Epoch : 84,  train loss : 0.00020824349485337734\n",
      "Epoch : 84,  val_loss : 0.00017518011736683547\n",
      "Epoch : 85,  train loss : 0.00016671160119585693\n",
      "Epoch : 85,  val_loss : 0.0002496769593562931\n",
      "Epoch : 86,  train loss : 0.00016189822054002434\n",
      "Epoch : 86,  val_loss : 0.00023654475808143616\n",
      "Epoch : 87,  train loss : 0.00017476428183726966\n",
      "Epoch : 87,  val_loss : 0.00015450453793164343\n",
      "Epoch : 88,  train loss : 0.00022608425933867693\n",
      "Epoch : 88,  val_loss : 0.00024215469602495432\n",
      "Epoch : 89,  train loss : 0.00023599425912834704\n",
      "Epoch : 89,  val_loss : 0.00019877507293131202\n",
      "Epoch : 90,  train loss : 0.00022116191394161433\n",
      "Epoch : 90,  val_loss : 0.0002083896251861006\n",
      "Epoch : 91,  train loss : 0.00024875058443285525\n",
      "Epoch : 91,  val_loss : 0.00015036266995593905\n",
      "Epoch : 92,  train loss : 0.00014178614947013557\n",
      "Epoch : 92,  val_loss : 0.00020575242524500936\n",
      "Epoch : 93,  train loss : 0.00023567077005282044\n",
      "Epoch : 93,  val_loss : 0.00018904960597865283\n",
      "Epoch : 94,  train loss : 0.00017278823361266404\n",
      "Epoch : 94,  val_loss : 0.00020089151803404093\n",
      "Epoch : 95,  train loss : 0.0001762991159921512\n",
      "Epoch : 95,  val_loss : 0.00014999501581769437\n",
      "Epoch : 96,  train loss : 0.000283252535155043\n",
      "Epoch : 96,  val_loss : 0.0002580283035058528\n",
      "Epoch : 97,  train loss : 0.0001948497083503753\n",
      "Epoch : 97,  val_loss : 0.00035074364859610796\n",
      "Epoch : 98,  train loss : 0.00019196790526621044\n",
      "Epoch : 98,  val_loss : 0.00021482181909959763\n",
      "Epoch : 99,  train loss : 0.00023469347797799855\n",
      "Epoch : 99,  val_loss : 0.00019949045963585377\n",
      "Epoch : 100,  train loss : 0.00022595925838686526\n",
      "Epoch : 100,  val_loss : 0.00027036419487558305\n"
     ]
    }
   ],
   "source": [
    "train_best_near=train(batch_size=64,epochs=100,lr=0.0001,train_val_split=0.3,scheduler=False,near=True)\n",
    "train_best_near.trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Loss on right half : 0.011029566272136759\n",
      "R2 score: 0.9282636140570228\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data_nearby=dataset_nearby_pixel(LPFe_Map,LPK_Map,LPTh_Map,LPTi_Map,albedo,mode=\"test\")\n",
    "test_loader_nearby=DataLoader(test_data_nearby,batch_size=1)\n",
    "loss_function=torch.nn.MSELoss()\n",
    "net_test=autoencoder()\n",
    "net_test=net_test.cuda()\n",
    "net_test.load_state_dict(torch.load(dir+\"/albedo_autoencoder_100_0.0001_64.pth\"))\n",
    "right_predicted_near=[]\n",
    "right_truth_near=[]\n",
    "total_loss_near=[]\n",
    "net_test=net_test.eval()\n",
    "for i,l in test_loader_nearby:\n",
    "    i=i.cuda()\n",
    "    l=l.cuda()\n",
    "    l=torch.reshape(l,(len(l),1))\n",
    "    output=net_test(i.float())\n",
    "    loss=loss_function(output,l.float())\n",
    "    loss=loss.cpu().item()\n",
    "    total_loss_near.append(np.sqrt(loss))\n",
    "    right_predicted_near.append(output.cpu().item())\n",
    "    right_truth_near.append(l.cpu().item())\n",
    "    \n",
    "\n",
    "print(\"RMSE Loss on right half :\",np.mean(total_loss_near))\n",
    "print(\"R2 score:\",r2_score(right_truth_near,right_predicted_near))\n",
    "residual_near=np.subtract(right_predicted_near,right_truth_near)\n",
    "plt.hist(residual_near,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e9e04228cc62b6d88a2d49485c7cafe926fad2c85f5988ecce6180f5d2d1170"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
